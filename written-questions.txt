Written Questions

Q1. Run the web crawler using the configurations located at src/main/config/written_question_1a.json and
    src/main/config/written_question_1b.json. The only difference between these configurations is that one always uses
    the sequential crawler and the other always uses the parallel crawler. Inspect the profile output in
    profileData.txt.
        If you are using a multi-processor computer, you should notice that SequentialWebCrawler#crawl and
        ParallelWebCrawler#crawl took about the same amount of time, but PageParserImpl#parse took much longer when run with
        the ParallelWebCrawler.
    Why did the parser take more time when run with ParallelWebCrawler?
	
	Parser takes more time because, the parallel web crawler visits more urls than the SequentialWebCrawler, and for each will call the parse for pages visited. 
	The result below demontrate it.
	{"wordCounts":{"data":437,"udacity":408,"learning":319,"with":272,"learn":215},"urlsVisited":26}
	Run at Thu, 17 Nov 2022 23:05:06 GMT
	com.udacity.webcrawler.SequentialWebCrawler#crawl took 0m 7s 84ms
	com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 7s 31ms

	{"wordCounts":{"udacity":2903,"data":2550,"with":1820,"your":1620,"program":1507},"urlsVisited":188}
	Run at Thu, 17 Nov 2022 23:02:42 GMT
	com.udacity.webcrawler.ParallelWebCrawler#crawl took 0m 8s 897ms
	com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 53s 591ms

Q2. Your manager ran your crawler on her old personal computer, using the configurations from Q1, and she notices that
    the sequential crawler actually outperforms the parallel crawler. She would like to know why.
    (a) Suggest one reason why the sequential web crawler was able to read more web pages than the parallel crawler.
        (Hint: Try setting "parallelism" to 1 in the JSON configs to simulate your manager's computer.)
		It is because in the old computer the parallel web crawler will use more resources in a single processor. 
    (b) Suggest one scenario in which the parallel web crawler will almost certainly perform better than the sequential
        crawler. Why will it perform better?
		In a computer with multi processors the Parallel Web Crawler will perform better than the sequential crawler. 
		Becauses Fork/join framework takes advantage of multiple processors by using all the available processing power to enhance the performance of application.
		In our case each sub task will be processed to a worker threads in a thread pool; 
		And as it implements work stealing technique, the optimization will be better so that the idle worker threads will find work to do.
		

Q3. Analyze your method profiler through the lens of Aspect Oriented Programming, by answering the following questions:
    (a) What cross-cutting concern is being addressed by the com.udacity.webcrawler.profiler.Profiler class?
	The cross-cutting concern in the Profiler is the performance profiled.
    (b) What are the join points of the Profiler in the web crawler program?
	The method crawl with the @Profiled annotation.

Q4. Identify three (3) different design patterns used in this project, and explain which interfaces, classes, and/or
    libraries use or implement those design patterns.
    For each pattern, name one thing about the pattern that you LIKED, and one thing you DISLIKED. If you did not like
    anything, you can name two things you disliked.
	
	Design Patterns:
	1. Builder is use in the CrawlerConfiguration class. 
	A Builder is a mutable factory that constructs the state of a to-be-created object, property by property, and then builds the object.
	I like the Builder pattern first, because the object created is immutable, and method chaining is more readable.
	
	2. Depency Injection is use in the WebCrawler class.
	Dependency Injection, or DI, is a design pattern that moves the creation of dependencies to outside of your code. 
	What I like in Dependency Injection, or DI, is that, instead of creating objects, you tell the DI framework to create the objects for you, and then you inject those objects into your class.
	
	3. Abstract Factory is use in the PageParserFactory class 
	Abstract Factory is a creational design pattern that lets you produce families of related objects without specifying their concrete classes.
	What I like is that Abstract Factory helps you control the classes of objects that an application creates.